---
title: Welfare
description: Fairness may occur when 'the maximum payoff received by any individual is not so much greater than the average welfare of every individual'. This definiton can help us quantify how efficiently we communicate value in distributed systems.
keywords:
  - understand
  - suave
  - theory
  - fairness
---

import Video from "@site/src/components/Video/Video.tsx";
import AlignItems from "@site/src/components/AlignItems/AlignItems.tsx";

# To be fair

1. How can we ensure that people can participate in the value they create by virtue of the way information about their intentions is acted on? 

    - _We can build systems that account for the time it takes to communicate price._

2. How can systems account for the time it takes to communicate price? 
    
    - _By cultivating shared, permissionless, open marketplaces in which it is profitable to make credible commitments about how you will act on information you receive._  

3. Will such systems be fair? 

    - _If we define fairness as "the maximum payoff received by any individual is not much greater than the average welfare of every individual", then an open marketplace might increase welfare because people are more likely to share their information with those who give them the greatest reward, which can only be done by those who both maximize revenue **and** distribute it well._

4. Is welfare a worthwhile goal? 

    - _Both 'fairness' and 'welfare' are laden terms with complex histories. However, using our quantifiable definition of fairness, we can instead ask, what do we expect the result of greater welfare to be? Our suggestion is that optimizing for welfare requires creating open and contestable power structures. Power is what this whole section revolves around, and quantifying welfare might help us test how [decentralized power is in any given system (next page)](/understand/power), as well as whether [your individual capacity to make meaning matches your buying power (previous page)](/understand/meaning)._

# Cui bono?

The claim we'll make on this page is that **we should anticipate how our messages move through the media we use so that no-one is unfairly manipulated**. We've already looked at how we communicate meaning and value, now we will turn to the time it takes for communication to occur.

Blockchains can be described as verifiable commitment devices. Such devices enable us to [craft prosocial coordination games](https://www.kernel.community/en/learn/module-1/dreamers/#an-infinite-stock-of-games). However, credible commitments do not totally remove uncertainty about the payoffs available to different actors, which can make it difficult to quantify whether a given application is fair in the sense we have defined it above. Uncertainty about payoffs largely stems from the period of time between each block in wich we cannot make commitments and achieve consensus (i.e. make them credible).

Intra-block time actually introduces a whole spectrum of uncertainty about the payoffs associated with actions taken on information received between blocks. Tarun Chitra explores this thoroughly in the video below:

<Video src="https://www.youtube-nocookie.com/embed/WYH7n4M016A?start=14626" title="Towards a Theory of MEV: Uncertainty" />

One of the key questions Tarun poses is, "How do we **achieve a balance** between maximizing revenue (which is good for the stability of systems, especially if they rely on arbitrage), and making sure that the expected welfare for all participants - on average - is not so far away from the maximum payoff?"

In order to analyze this question, Tarun starts by looking at welfare in the context of specific applications, rather than the system as a whole. In particular, he shows that applications all have a payoff function, which depends on a set of permutations (i.e. how you order transactions). Such payoff functions can be "smooth" (i.e. the order of transactions doesn't lead to significant differences in the payoff available if there is sufficient liquidity). He proved this in the context of AMMs, where the maximum and expected payoff are only separated multiplicatively by a factor of `log(n)`.

However, payoff functions in the context of _liquidations_ are discontinuous. If the price drops below a certain point, everything happens, but until such time, nothing happens. So, if you can permute the transactions from a specific application such that the order results in the price dropping below the point at which a liquidation occurs, then you can achieve payoffs which differ significantly from the average. This is unfair, by our definition. It also means that liquidations are the fundamental basis for payoffs: any payoff function can be written as a sum of liquidation games. It's worth reflecting deeply on this point:

> **People taking leverage serves as the basis for the set of all payoffs**. 

Derivatives really are weapons of mass destruction. Tarun's proof - which shows that the maximum and expected payoff in the context of liquidation games is separated multiplicatively by a factor of `n!` - is powerful becauseit explains **why** derivatives are destructive. Everyone knows leveraged positions are more risky. However, when we look through the lens of MEV, we can see that taking a leveraged position creates systemic risk because it increases the chance that the payoff sophisticated users can get will differ extremely from the average, which shifts the global incentive structure toward private ownership (manipulation for individual profit) and away from shared stewardship (reciprocity for mutual flourishing).

# Fairier transforms

Tarun asks another question, based on the above: "Is there a way in which, given a payoff function, we can determine the difference between the average and maximum payoff such that we can provide some guarantees on fairness?" He suggests that an analogue of Fourier transforms can assist us do exactly this.

If all payoffs are the sum of liquidation games, we can decompose them in the same way we can separate out different frequencies in a complex waveform. The "high frequency" waves are equivalent to sets of transactions in which there are very few permutations that can trigger a liquidation. The "low frequency" waves are equivalent to sets of transactions where many different permutations can trigger a liquidation. This allows us to analyze how constrained we are in realising a particular payoff (lots of high frequencies `=` constrained, lots of low frequencies `=` not constrained). 

We can go further by looking at the Fourier coefficient. If a set of permutations is large, and the Fourier coefficient over that set is large, then the value of the payoff function approaches the average across all permutations. If the set is small, but has a very large coefficient, the payoff approaches the maximum value. Tarun also introduces the Uncertainty Principle which, in this case, states that if you make a function really sharp (highly specified/exact), the Fourier transform of that function cannot also be sharp. This insight provides a means to bound the relationships between expected and average payoffs. 

In review:

1. Liquidations generate all payoff functions
2. The Fourier transform provides a way of going from a given payoff to that payoff represented in the basis of all liquidations
3. The Uncertainty Principle provides a bound on the expected value over the maximum value

These three facts together give us an explicit way to quantify welfare!

Note that the "welfare" we are quantifying here is premised on a very context-specific definition: [there is no universal definition of fairness](https://www.youtube.com/live/krlAqKsdLkw?si=i-L_OZFQTHYJ7y1C&t=1439), just as there is no "ultimate meaning", which is why MEV will always exist to some degree. However, if each application can order transactions based on the above general mathematical insights applied to their specific context, we can meaningfully reduce MEV.

In practice, this means that to avoid maximally unfair payoffs, we need to have a sufficiently large set of possible orderings _and_ there needs to be a large degree of overlap between the possible orderings and the Fourier coefficient. One way to achieve this is an **open marketplace for mechanisms** at the application level. 

# Generally suave

This is exactly what SUAVE is: an open marketplace for applications that act on the information in transactions in verifiable ways, such that the incentives in the system tend towards general welfare and away from the centralization of power.

It's important to understand the _general_ nature of SUAVE. As we have stressed, MEV is a general phenomenon which arises from the time it takes to communicate. People have been aware of this for centuries, but no-one has attempted to build systems that can account for it, because no-one had the sort of verifiable commitment devices (blockchains) we enjoy.

Bitcoin represents a new medium with/in which we can coordinate the creation and distribution of money. Ethereum represents a new medium with/in which we can coordinate _in general_. However, the commitments this requires are public, which means that the information they contain can be manipulated in the time it takes to verify a new block, creating unfair outcomes for those seeking to coordinate.

**SUAVE represents a new medium with/in which we can coordinate how everyone coordinates**. SUAVE is the metacognition for every other coordination system. And we do mean every other coordination system: everything from blockchains to traditional exchanges to Google AdWords suffers from some version of MEV.

<AlignItems horizontal="center" vertical="center" sideMargin={0}>
<div>
  <img alt="Fairier Transformers" src={require('/static/img/fairier-transformers.png').default} />
</div>
</AlignItems>